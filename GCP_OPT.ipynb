{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized CP (GCP) Tensor Decomposition\n",
    "\n",
    "```\n",
    "Copyright 2022 National Technology & Engineering Solutions of Sandia,\n",
    "LLC (NTESS). Under the terms of Contract DE-NA0003525 with NTESS, the\n",
    "U.S. Government retains certain rights in this software.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "This document outlines usage and examples for the generalized CP (GCP) tensor decomposition implmented in `pyttb.gcp_opt`. GCP allows alternate objective functions besides sum of squared errors, which is the standard for CP. The code support both dense and sparse input tensors, but the sparse input tensors require randomized optimization methods.\n",
    "\n",
    "GCP is described in greater detail in the manuscripts:\n",
    "* D. Hong, T. G. Kolda, J. A. Duersch, Generalized Canonical Polyadic Tensor Decomposition, SIAM Review, 62:133-163, 2020, https://doi.org/10.1137/18M1203626\n",
    "* T. G. Kolda, D. Hong, Stochastic Gradients for Large-Scale Tensor Decomposition. SIAM J. Mathematics of Data Science, 2:1066-1095, 2020, https://doi.org/10.1137/19m1266265"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "The idea of GCP is to use alternative objective functions. As such, the most important thing to specify is the objective function.\n",
    "\n",
    "The command \n",
    "```\n",
    "M = ttb.gcp_opt(data=X, rank=rank, objective=Objectives.<TYPE>, optimizer=<OPT>)\n",
    "``` \n",
    "computes an estimate of the best rank-|R| generalized CP (GCP) decomposition of the tensor `X` for the specified generalized loss function specified by `<TYPE>` solved with optimizer `<OPT>`. The input `X` can be a tensor or sparse tensor. The result `M` is a Kruskal tensor. \n",
    "\n",
    "Predefined objective functions are:\n",
    "\n",
    "* `GAUSSIAN`: Gaussian distribution (see also `cp_als` and `cp_opt`)\n",
    "* `BERNOULLI_ODDS`: Bernoulli distribution for binary data\n",
    "* `BERNOULLI_LOGIT`: Bernoulli distribution for binary data with log link\n",
    "* `POISSON`: Poisson distribution for count data (see also `cp_apr`)\n",
    "* `POISSON_LOG`: Poisson distribution for count data with log link\n",
    "* `RAYLEIGH`: Rayleigh distribution for nonnegative continuous data\n",
    "* `GAMMA`: Gamma distribution for nonnegative continuous data\n",
    "* `HUBER`: Similar to Gaussian but robust to outliers\n",
    "* `NEGATIVE_BINOMIAL`: Models the number of trials required before we experience some number of failures. May be a useful alternative when Poisson is overdispersed.\n",
    "* `BETA`: Generalizes exponential family of loss functions.\n",
    "\n",
    "Alternatively, a user can supply one's own objective function as a tuple of `function_handle`, `gradient_handle`, and `lower_bound`.\n",
    "\n",
    "Supported optimizers are:\n",
    "* `LBFGSB`: bound-constrained limited-memory BFGS (L-BFGS-B). L-BFGS-B can only be used for dense tensors.\n",
    "* `SGD`: Stochastic gradient descent (SGD). Can be used with both dense and sparse tensors.\n",
    "* `Adagrad`: Adaptive gradients SGD method. Can be used with both dense and sparse tensors.\n",
    "* `Adam`: Momentum-based SGD method. Can be used with both dense and sparse tensors.\n",
    "\n",
    "Each methods has parameters, which are described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying Missing or Incomplete Data Using the Mask Option\n",
    "If some entries of the tensor are unknown, the method can mask off that data during the fitting process. To do so, specify a *mask* tensor `W` that is the same size as the data tensor `X`. The mask tensor should be 1 if the entry in `X` is known and 0 otherwise. The call is \n",
    "```\n",
    "M = ttb.gcp_opt(data=X, rank=rank, objective=Objectives.<TYPE>, optimizer=LBFGSB, mask=W)\n",
    "```\n",
    "Note: that `mask` isn't supported for stochastic solves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver options\n",
    "Defaults are listed in brackets {}\n",
    "\n",
    "Common options that can be passed to `pyttb.gcp_opt()` include:\n",
    "\n",
    "* `init`: Initial solution to the problem {\"random\"}.\n",
    "* `printitn`: Controls verbosity of printing throughout the solve: print every n iterations; 0 for no printing.\n",
    "* `sampler`: Class that defined sampling strategy for stochastic solves.\n",
    "\n",
    "## Other Options\n",
    "In addition to the options above, the behavior of optimizers can be affected by constructing the optimizer with the following optional parameters.\n",
    "\n",
    "### Specifying L-BFGS-B Parameters\n",
    "* `m`: {None}\n",
    "* `factr`: Tolerance on the change on the objective value. Defaults to 1e7, which is multiplied by machine epsilon. {1e7}\n",
    "* `pgtol`: Projected gradient tolerance, defaults to 1e-4 times total tensor size. It can sometimes be useful to increase or decrease `pgtol` depending on the objective function and size of the tensor. {None}\n",
    "* `epsilon`: {None}\n",
    "* `iprint`: {None}\n",
    "* `disp`: {None}\n",
    "* `maxfun`: {None}\n",
    "* `maxiter`: {1000}\n",
    "* `callback`: {None}\n",
    "* `maxls`: {None}\n",
    "\n",
    "### Specifying SGD, Adagrad, and ADAM Parameters\n",
    "There are a number of parameters that can be adjusted for SGD and ADAM.\n",
    "\n",
    "#### Stochastic Gradient\n",
    "There are three different sampling methods for computing the stochastic gradient:\n",
    "\n",
    "* Uniform - Entries are selected uniformly at random. Default for dense tensors.\n",
    "* Stratified - Zeros and nonzeros are sampled separately, which is recommended for sparse tensors. Default for sparse tensors.\n",
    "* Semi-Stratified - Modification to stratified sampling that avoids rejection sampling for better efficiency at the cost of potentially higher variance.\n",
    "\n",
    "The options corresponding to these are as follows.\n",
    "\n",
    "* `gradient_sampler`: Type of sampling to use for stochastic gradient. Specified by setting `pyttb.gcp.samplers.<SAMPLER>`. Predefined options for `<SAMPLER>` are:\n",
    "    * `Samplers.UNIFORM`: default for dense.\n",
    "    * `Samplers.STRATIFIED`: default for sparse.\n",
    "    * `Samplers.SEMISTRATIFIED`\n",
    "* `gradient_samples`: The number of samples for stochastic gradient can be specified as either an `int` or a `StratifiedCount` object. This should generally be *O(sum(size(X))R)*, where *R* is the target rank. For the uniform sampler, only an `int` can be provided. For the stratified or semi-stratified sampler, this can be two numbers `a, b` provided as arguments to a `pyttb.gcp.samplers.StratifiedCount(a, b)` object. The first `a` is the number of nonzero samples and the second `b` is the number of zero samples. If only one number is specified, then this is used as the number for both nonzeros and zeros, and the total number of samples is 2x what is specified.\n",
    "\n",
    "#### Estimating the Function.\n",
    "\n",
    "We also use sampling to estimate the function value.\n",
    "\n",
    "* `function_sampler`: This can be any of the three samplers specified above or a custom function handle. The custom function handle is primarily useful in reusing the same sampled elements across different tests.\n",
    "* `function_samples`: Number of samples to estimate function. As before, the number of samples for estimating the function can be specified as either an `int` or a `StratifiedCount` object. This should generally be somewhat large since we want this sample to generate a reliable estimate of the true function value.\n",
    "\n",
    "Creating the sampler takes two additional options:\n",
    "* `max_iters`: Maximum number of iterations to normalize number of samples. {1000}\n",
    "* `over_sample_rate`: Ratio of extra samples to take to account for bad draws. {1.1}\n",
    "\n",
    "There are some other options that are needed for SGD: the learning rate and a decrease schedule. Our schedule is very simple - we decrease the rate if there is no improvement in the approximate function value after an epoch. After a specified number of decreases (`max_fails`), we quit.\n",
    "\n",
    "* `rate`: Rate of descent, proportional to step size. {1e-3}\n",
    "* `decay`: How much to decrease step size on failed epochs. {0.1}\n",
    "* `max_fails`: How many failed epochs before terminating the solve. {1}\n",
    "* `epoch_iters`:  Number of steps to take per epoch. {1000}\n",
    "* `f_est_tol`: Tolerance for function estimate changes to terminate solve. {-inf}\n",
    "* `max_iters`: Maximum number of epochs. {1000}\n",
    "* `printitn`: Controls verbosity of information during solve. {1}\n",
    "\n",
    "There are some options that are specific to ADAM and generally needn't change:\n",
    "* `beta_1`: Adam-specific momentum parameter beta_1. {0.9}\n",
    "* `beta_2`: Adam-specific momentum parameter beta_2. {0.999}\n",
    "* `epsilon`: Adam-specific momentum parameter to avoid division by zero. {1e-8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pyttb as ttb\n",
    "import numpy as np\n",
    "from pyttb.pyttb_utils import tt_tenfun\n",
    "\n",
    "from pyttb.gcp.fg_setup import function_type, setup\n",
    "from pyttb.gcp.handles import Objectives\n",
    "from pyttb.gcp.optimizers import LBFGSB, SGD, Adagrad, Adam\n",
    "from pyttb.gcp.samplers import GCPSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ttb.tenones((2, 2))\n",
    "X[0, 1] = 0.0\n",
    "X[1, 0] = 0.0\n",
    "rank = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run GCP-OPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final fit: 0.24303789878041926 (for comparison to f(x) in CP-ALS)\n",
      "\n",
      "CPU times: user 6.15 ms, sys: 3.57 ms, total: 9.72 ms\n",
      "Wall time: 6.72 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Select Gaussian objective\n",
    "objective = Objectives.GAUSSIAN\n",
    "\n",
    "# Select LBFGSB solver with 2 max iterations\n",
    "optimizer = LBFGSB(maxiter=2)\n",
    "\n",
    "# Compute rank-2 GCP approximation to X with GCP-OPT\n",
    "# Return result, initial guess, and runtime information\n",
    "np.random.seed(0) # Creates consistent initial guess\n",
    "result_lbfgs, initial_guess, info_lbfgs = ttb.gcp_opt(data=X, rank=rank, objective=objective, optimizer=optimizer)\n",
    "\n",
    "print(f\"\\nFinal fit: {1 - np.linalg.norm((X-result_lbfgs.full()).double())/X.norm()} (for comparison to f(x) in CP-ALS)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to CP-ALS, which should usually be faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CP_ALS:\n",
      " Iter 0: f = 9.999999e-01 f-delta = 1.0e+00\n",
      " Iter 1: f = 9.999999e-01 f-delta = 3.0e-09\n",
      " Final f = 9.999999e-01\n",
      "\n",
      "Final fit: 0.9999999999999982 (for comparison to f(x) in GCP-OPT)\n",
      "\n",
      "CPU times: user 6.26 ms, sys: 4.44 ms, total: 10.7 ms\n",
      "Wall time: 4.68 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_als, _, info_als = ttb.cp_als(input_tensor=X, rank=rank, maxiters=2, init=initial_guess)\n",
    "print(f\"\\nFinal fit: {1 - np.linalg.norm((X-result_als.full()).double())/X.norm()} (for comparison to f(x) in GCP-OPT)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's try is with the ADAM functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final fit: 0.9999888356995471 (for comparison to f(x) in GCP-OPT & CP-ALS)\n",
      "\n",
      "CPU times: user 1.68 s, sys: 27.6 ms, total: 1.7 s\n",
      "Wall time: 1.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Select Gaussian objective\n",
    "objective = Objectives.GAUSSIAN\n",
    "\n",
    "# Select LBFGSB solver with 2 max iterations\n",
    "optimizer = Adam(max_iters=2)\n",
    "\n",
    "# Compute rank-2 GCP approximation to X with GCP-OPT\n",
    "# Return result, initial guess, and runtime information\n",
    "result_adam, _, info_adam = ttb.gcp_opt(data=X, rank=rank, objective=objective, optimizer=optimizer, init=initial_guess)\n",
    "print(f\"\\nFinal fit: {1 - np.linalg.norm((X-result_adam.full()).double())/X.norm()} (for comparison to f(x) in GCP-OPT & CP-ALS)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime information from `gcp_opt_lbfgs`: \n",
      "{'grad': array([-0.43201408,  0.06840212, -0.81661589,  0.10173867,  0.21841733,\n",
      "       -0.55025692,  0.20614942, -0.21326614]), 'task': 'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT', 'funcalls': 6, 'nit': 2, 'warnflag': 1, 'final_f': 1.1459832453655254}\n",
      "\n",
      "Runtime information from `cp_als`: \n",
      "{'params': (0.0001, 2, 1, [0, 1]), 'iters': 1, 'normresidual': 1.97686242482388e-07, 'fit': 0.9999998602147174}\n",
      "\n",
      "Runtime information from `gcp_opt_adam`: \n",
      "{'f_est_trace': array([0.60997726, 0.05540865]), 'step_trace': array([0.   , 0.001]), 'time_trace': array([0.00103578, 0.84131715]), 'n_epoch': 1}\n"
     ]
    }
   ],
   "source": [
    "# Inspect runtime information from each run\n",
    "print(f\"Runtime information from `gcp_opt_lbfgs`: \\n{info_lbfgs}\")\n",
    "print(f\"\\nRuntime information from `cp_als`: \\n{info_als}\")\n",
    "print(f\"\\nRuntime information from `gcp_opt_adam`: \\n{info_adam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an example Rayleigh tensor model and data instance.\n",
    "Consider a tensor that is Rayleigh-distribued. This means its entries are all nonnegative. First, we generate such a tensor with low-rank structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(65)\n",
    "rank = 3\n",
    "shape = (60, 70, 80)\n",
    "ndims = len(shape)\n",
    "\n",
    "# Create factor matrices that correspond to smooth sinusidal factors\n",
    "U = []\n",
    "for k in np.arange(ndims):\n",
    "    V = 1.1 + np.cos((2 * np.pi/shape[k] * np.arange(shape[k])[:,np.newaxis]) * np.arange(1,ndims+1))\n",
    "    U.append(V[:, rng.permutation(rank)])\n",
    "    \n",
    "M_true = ttb.ktensor(U).normalize()\n",
    "\n",
    "X = ttb.tensor(rng.rayleigh()*M_true.double())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run GCP-OPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          630     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.10614D+06    |proj g|=  1.07730D+08\n",
      "  ys=-2.963E+08  -gs= 7.250E+08 BFGS update SKIPPED\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  630      4     36    181     1   174   1.091D+08   2.417D+06\n",
      "  F =   2417245.8361591692     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "\n",
      "Final fit: -10.634343794707151\n",
      "\n",
      "CPU times: user 9.5 s, sys: 5.59 s, total: 15.1 s\n",
      "Wall time: 1.95 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " Bad direction in the line search;\n",
      "   refresh the lbfgs memory and restart the iteration.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Select Gaussian objective\n",
    "objective = Objectives.RAYLEIGH\n",
    "\n",
    "# Select LBFGSB solver\n",
    "optimizer = LBFGSB(iprint=10)\n",
    "\n",
    "# Compute rank-3 GCP approximation to X with GCP-OPT\n",
    "result_lbfgs, initial_guess, info_lbfgs = ttb.gcp_opt(data=X, rank=rank, objective=objective, optimizer=optimizer)\n",
    "\n",
    "print(f\"\\nFinal fit: {1 - np.linalg.norm((X-result_lbfgs.full()).double())/X.norm()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's try is with the scarce functionality - this leaves out all but 10% of the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer \u001b[39m=\u001b[39m Adam()\n\u001b[1;32m      7\u001b[0m \u001b[39m# Compute rank-3 GCP approximation to X with GCP-OPT\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m result_adam, initial_guess, info_adam \u001b[39m=\u001b[39m ttb\u001b[39m.\u001b[39;49mgcp_opt(data\u001b[39m=\u001b[39;49mX, rank\u001b[39m=\u001b[39;49mrank, objective\u001b[39m=\u001b[39;49mobjective, optimizer\u001b[39m=\u001b[39;49moptimizer)\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mFinal fit: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m1\u001b[39m\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mnp\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm((X\u001b[39m-\u001b[39mresult_adam\u001b[39m.\u001b[39mfull())\u001b[39m.\u001b[39mdouble())\u001b[39m/\u001b[39mX\u001b[39m.\u001b[39mnorm()\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Tamales/pyttb/pyttb/gcp_opt.py:112\u001b[0m, in \u001b[0;36mgcp_opt\u001b[0;34m(data, rank, objective, optimizer, init, mask, sampler, printitn)\u001b[0m\n\u001b[1;32m    109\u001b[0m     logging\u001b[39m.\u001b[39minfo(welcome_msg)\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(optimizer, StochasticSolver):\n\u001b[0;32m--> 112\u001b[0m     result, info \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39;49msolve(\n\u001b[1;32m    113\u001b[0m         M0, data, function_handle, gradient_handle, lower_bound, sampler\n\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    115\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(mask, ttb\u001b[39m.\u001b[39mtensor):\n",
      "File \u001b[0;32m~/Tamales/pyttb/pyttb/gcp/optimizers.py:169\u001b[0m, in \u001b[0;36mStochasticSolver.solve\u001b[0;34m(self, initial_model, data, function_handle, gradient_handle, lower_bound, sampler)\u001b[0m\n\u001b[1;32m    157\u001b[0m g_est \u001b[39m=\u001b[39m estimate(\n\u001b[1;32m    158\u001b[0m     model,\n\u001b[1;32m    159\u001b[0m     g_subs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     crng\u001b[39m=\u001b[39msampler\u001b[39m.\u001b[39mcrng,\n\u001b[1;32m    166\u001b[0m )\n\u001b[1;32m    168\u001b[0m \u001b[39m# Check for inf\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39many(np\u001b[39m.\u001b[39;49misinf(g_est)):\n\u001b[1;32m    170\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    171\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInfinite gradient encountered! (epoch = \u001b[39m\u001b[39m{\u001b[39;00mn_epoch\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39miter = \u001b[39m\u001b[39m{\u001b[39;00miteration\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m     )\n\u001b[1;32m    174\u001b[0m model\u001b[39m.\u001b[39mfactor_matrices, step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_step(\n\u001b[1;32m    175\u001b[0m     model, g_est, lower_bound\n\u001b[1;32m    176\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Select Gaussian objective\n",
    "objective = Objectives.RAYLEIGH\n",
    "\n",
    "# Select Adam solver\n",
    "optimizer = Adam()\n",
    "\n",
    "# Compute rank-3 GCP approximation to X with GCP-OPT\n",
    "result_adam, initial_guess, info_adam = ttb.gcp_opt(data=X, rank=rank, objective=objective, optimizer=optimizer)\n",
    "\n",
    "print(f\"\\nFinal fit: {1 - np.linalg.norm((X-result_adam.full()).double())/X.norm()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean tensor.\n",
    "The model will predict the odds of observing a 1. Recall that the odds related to the probability as follows. If $p$ is the probability and $r$ is the odds, then $r = p / (1-p)$. Higher odds indicates a higher probability of observing a one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(7639)\n",
    "rank = 3\n",
    "shape = (60, 70, 80)\n",
    "ndims = len(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the underlying model tensor has factor matrices with only a few \"large\" entries in each column. The small entries should correspond to a low but nonzero entry of observing a 1, while the largest entries, if multiplied together, should correspond to a very high likelihood of observing a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probrange = np.array([0.01, 0.99])\n",
    "oddsrange = probrange / (1-probrange)\n",
    "smallval = (np.power(np.min(oddsrange)/rank, (1/ndims)))\n",
    "largeval = (np.power(np.max(oddsrange)/rank, (1/ndims)))\n",
    "\n",
    "A = []\n",
    "for k in np.arange(ndims):\n",
    "    A.append(smallval * np.ones((shape[k], rank)))\n",
    "    nbig = 5\n",
    "    for j in np.arange(rank):\n",
    "        p = rng.permutation(shape[k])\n",
    "        A[k][p[:nbig-1],j] = largeval\n",
    "        \n",
    "M_true = ttb.ktensor(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert K-tensor to an observed tensor. Get the model values, which correspond to odds of observing a 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mfull = M_true.double()\n",
    "# Convert odds to probabilities\n",
    "Mprobs = Mfull / (1+Mfull)\n",
    "# Flip a coin for each entry, with the probability of observing a one\n",
    "# dictated by Mprobs\n",
    "Xfull = 1.0*(ttb.tenrand(shape) < Mprobs)\n",
    "# Convert to sparse tensor, real-valued 0/1 tensor since it was constructed\n",
    "# to be sparse\n",
    "X = Xfull.to_sptensor();\n",
    "print(f\"Proportion of nonzeros in X is {100*X.nnz / np.prod(shape):.2f}%\\n\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just for fun, let's visualize the distribution of the probabilities in the model tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(Mprobs.tolist()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call GCP_OPT on the full tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Select Gaussian objective\n",
    "objective = Objectives.BERNOULLI_ODDS\n",
    "\n",
    "# Select LBFGSB solver\n",
    "optimizer = LBFGSB(iprint=25)\n",
    "\n",
    "# Compute rank-3 GCP approximation to X with GCP-OPT\n",
    "result_lbfgs, initial_guess, info_lbfgs = ttb.gcp_opt(data=Xfull, rank=rank, objective=objective, optimizer=optimizer, printitn=25)\n",
    "\n",
    "print(f\"\\nFinal fit: {1 - np.linalg.norm((X-result_lbfgs.full()).double())/X.norm()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCP-OPT as sparse tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Select Gaussian objective\n",
    "objective = Objectives.BERNOULLI_ODDS\n",
    "\n",
    "# Select LBFGSB solver\n",
    "optimizer = Adam()\n",
    "\n",
    "# Compute rank-3 GCP approximation to X with GCP-OPT\n",
    "result_adam, initial_guess, info_adam = ttb.gcp_opt(data=X, rank=rank, objective=objective, optimizer=optimizer, printitn=25)\n",
    "\n",
    "print(f\"\\nFinal fit: {1 - np.linalg.norm((X-result_adam.full()).double())/X.norm()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and test a Poisson count tensor.\n",
    "\n",
    "We follow the general procedure outlined by E. C. Chi and T. G. Kolda, On Tensors, Sparsity, and Nonnegative Factorizations, arXiv:1112.2414 [math.NA], December 2011 (http://arxiv.org/abs/1112.2414)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the size and rank\n",
    "sz = (10, 8, 6)\n",
    "R = 5\n",
    "\n",
    "# Generate factor matrices with a few large entries in each column\n",
    "# this will be the basis of our solution.\n",
    "np.random.seed(0) # Set seed for reproducibility\n",
    "A = []\n",
    "for n in range(len(sz)):\n",
    "    A.append(np.random.uniform(size=(sz[n], R)))\n",
    "    for r in range(R):\n",
    "        p = np.random.permutation(sz[n])\n",
    "        nbig = round((1/R)*sz[n])\n",
    "        A[-1][p[0:nbig], r] *= 100\n",
    "weights = np.random.uniform(size=(R,))\n",
    "S = ttb.ktensor(A, weights)\n",
    "S.normalize(sort=True, normtype=1)\n",
    "\n",
    "X = S.to_tensor()\n",
    "X.data = np.floor(np.abs(X.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function for Poisson negative log likelihood with identity link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Select Gaussian objective\n",
    "objective = Objectives.POISSON\n",
    "\n",
    "# Select LBFGSB solver\n",
    "optimizer = Adam()\n",
    "\n",
    "# Compute rank-3 GCP approximation to X with GCP-OPT\n",
    "result_adam, initial_guess, info_adam = ttb.gcp_opt(data=X, rank=rank, objective=objective, optimizer=optimizer, printitn=25)\n",
    "\n",
    "print(f\"\\nFinal fit: {1 - np.linalg.norm((X-result_adam.full()).double())/X.norm()}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
